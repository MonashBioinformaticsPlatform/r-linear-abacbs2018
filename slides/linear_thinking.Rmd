---
title: "Linear models in R (slideshow)"
subtitle: "Bioconductor Training Day 2018"
author: "Paul Harrison @paulfharrison"
output: 
    ioslides_presentation:
        widescreen: true
        smaller: true
        css: style.css
        incremental: true
---

```{r include=F}
knitr::opts_chunk$set(fig.width=5, fig.height=5)
```

## Introduction

* Hypothesis: lots of people mostly use R for Bioconductor
* R has a lot of other cool stuff in it
* A lot of the cool stuff in R has to do with **linear models**
* Bioconductor packages may assume users are familiar with this



## Linear models

"linear model" "general linear model" "linear predictor" "regression" "multiple regression" "multiple linear regression" "multiple linear regression model"

Learn to predict a response variable as

* a straight line relationship with a predictor variable
* or more than one predictor variable
* actually it doesn't have to be a straight line
* some of the predictors could be categorical (ANOVA)
* and there could be interactions

<br/>

* test which variables and interactions the data supports as predictors
* give a confidence interval on any estimates


## Linear models in R

<div style="float: right; width: 25%; padding 2em;">
```{r echo=F,out.width="100%",fig.align="left"}
knitr::include_graphics("../figures/chambers_and_hastie.jpg")
```
</div>

Many features of the S language (predecessor to R) were created to support working with linear models and its generalizations:

<br>

>* `data.frame` type introduced to hold data for modelling.

>* `factor` type introduced to hold categorical data.

>* `~` formula syntax specify terms in models.

>* Manipulation of "S3" objects holding fitted models.

>* Rich set of diagnostic visualization functions.

<br>

Primary reference is "Statistical models in S".


## Linear maths and R

<!--
Will give everything today in English, in maths, and in R code.
-->

We will be using vectors and matrices extensively today.

In mathematics, we usually treat a vector as a matrix with a single column. In R, they are two different types. <span class="tip">*<span class="tiptext">R also makes a distinction between `matrix` and `data.frame` types. There is a good chance you have used data frames but not matrices before now.<br><br>Matrices contain all the same type of value, typically numeric, whereas data frames can have different types in different columns.</span></span>

A matrix can be created from a vector using `matrix`, or from multiple vectors with `cbind` (bind columns) or `rbind` (bind rows).

Matrix transpose exchanges rows and columns. In maths it is indicated with a small t, eg $a^\top$. In R use the `t` function, eg `t(a)`.

$$
a = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix} 
\quad \quad a^\top = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} 
$$

## Linear maths and R

Taking the **dot product** of two vectors we multiply corresponding elements together and add the results to obtain a total.

In mathematical notation:

$$
a^\top b = \sum_{i=1}^{n} a_i b_i
$$

In R:

```{r eval=F}
sum(a*b)
```

## Linear maths and R

Taking the product of a matrix $X$ and a vector $a$ with length matching the number of columns, the result is a vector containing the dot product of each row of the matrix $X$ with the vector $a$.

$$
Xa
$$

In R:

```{r eval=F}
X %*% a
```

(It's also possible to multiply two matrices with `%*%` but we won't need this today.)


## Geometry

<div style="float: left; width: 25%; padding: 2em">
The dot product is our ruler and set-square.

```{r echo=F,out.width="100%",fig.align="left"}
knitr::include_graphics("../figures/Triangle-Ruler.png")
```
</div>

### Lengths

A vector can be thought of as an arrow in a space.

The dot product of a vector with itself $a^\top a$ is the square of its length. Pythagorus!

### Right angles

Two vectors at right angles have a dot product of zero. They are **orthogonal**.


## Geometry - subspaces

> * A line in two or more dimensions, passing through the origin.
> * A plane in three or more dimensions, passing through the origin.
> * A point at the origin.

These are all examples of **subspaces**.

Think of all the vectors that could result from multiplying a matrix $X$ with some arbitrary vector. 

If the matrix $X$ has $n$ rows and $p$ columns, we obtain an (at most) $p$-dimensional **subspace** within an $n$-dimensional space.

A subspace has an **orthogonal subspace** with $n-p$ dimensions. All the vectors in a subspace are orthogonal to all the vectors in its orthogonal subspace.


##

Do section: Vectors and matrices


## Models

A **model** can be used to predict a **response** variable based on a set of **predictor** variables. <span class="tip">*<span class="tiptext">Alternative terms: depedent variable, independent variables.<br><br>We are using causal language, but really only describing an association.</span></span>


The prediction will usually be imperfect, due to random noise.

<br>



## Linear model

A **response** $y$ is produced based on $p$ **predictors** $x_j$ plus noise $\varepsilon$ ("epsilon"):

$$ y = \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon $$

The model has $p$ **terms**, plus a noise term. The model is specified by the choice of **coefficients** $\beta$ ("beta").

This can also be written as a dot product:

$$ y = \beta^\top x + \varepsilon $$

<br>

The noise is assumed to be normally distributed with standard deviation $\sigma$ ("sigma") (i.e. variance $\sigma^2$):

$$ \varepsilon \sim \mathcal{N}(0,\sigma^2) $$

<br>

Typically $x_1$ will always be 1, so $\beta_1$ is a constant term in the model. We still count it as one of the $p$ predictors. <span class="tip">*<span class="tiptext">This matches what R does, but may differ from other presentations!</span></span>


## Linear model in R code

For vector of coefficients `beta` and vector of predictors in some particular case `x`, the most probable outcome is:

```{r eval=F}
y_predicted <- sum(beta*x)
```

A simulated possible outcome can be generated by adding random noise:

```{r eval=F}
y_simulated <- sum(beta*x) + rnorm(1, mean=0, sd=sigma)
```

But where do the coefficients $\beta$ come from?


## Model fitting -- estimating $\beta$

Say we have a observed $n$ responses $y_i$ with corresponding vectors of predictors $x_i$: 

$$ 
\begin{align}
y_1 &= \beta_1 x_{1,1} + \beta_2 x_{1,2} + \dots + \beta_p x_{1,p} + \varepsilon_1 \\
y_2 &= \beta_1 x_{2,1} + \beta_2 x_{2,2} + \dots + \beta_p x_{2,p} + \varepsilon_2 \\
    & \dots \\
y_n &= \beta_1 x_{n,1} + \beta_2 x_{n,2} + \dots + \beta_p x_{n,p} + \varepsilon_n 
\end{align}
$$

This is conveniently written in terms of a vector of responses $y$ and matrix of predictors $X$:

$$ y = X \beta + \varepsilon $$

Each response is assumed to contain the same amount of noise:

$$ \varepsilon_i \sim \mathcal{N}(0,\sigma^2) $$

## Model fitting -- estimating $\beta$ with geometry

$$ y = X \beta + \varepsilon $$
$$ \varepsilon_i \sim \mathcal{N}(0,\sigma^2) $$

**Maximum Likelihood**<span class="tip">*<span class="tiptext">"Likelihood" has a technical meaning in statistics: it is the probability of the *data* given the model parameters. This is backwards from normal English usage.</span></span> estimate:<br>
We choose $\hat\beta$ to maximize the likelihood of $y$.

A very nice consequence of assuming normally distributed noise is that the vector $\varepsilon$ has a spherical distribution, so the choice of $\hat\beta$ making $y$ most likely is the one that places $X \hat\beta$ nearest to $y$

Distance is the square root of the sum of squared differences in each dimension, so this is also called a **least squares** estimate. We choose $\hat \beta$ to minimize $\hat \varepsilon^\top \hat \varepsilon$.

$$ \hat \varepsilon = y - X \hat \beta $$


## Model fitting -- estimating $\beta$ with geometry

<div style="float: right; width: 15em;">
Imagine an experiment in which two noisy measurements of something are made.

Imagine many runs of this experiment.

The runs form a fuzzy circular cloud around the (noise-free) truth.
</div>

```{r echo=F}
source("../diagram.R")
begin()
many_exp()
the_truth()
```

## Model fitting -- estimating $\beta$ with geometry

```{r echo=F}
begin()
one_exp()
```


## Model fitting -- estimating $\beta$ with geometry

<div style="float: right; width: 15em;">
```{r}
y <- c(3,5)
fit <- lm(y ~ 1)

coef(fit)
predict(fit)
residuals(fit)
```
</div>

```{r echo=F}
begin()
to_predict()
```

## Model fitting -- estimating $\sigma$

<div style="float: right; width: 15em;">
The coefficient is wrong, but is our best estimate. It is an unbiassed estimate.

The residuals vector must be orthogonal to the subspace of possible predictions so it is too short, but we can correct for this to get an unbiassed estimate of the variance.

$$ \hat \sigma^2 = { \hat\varepsilon^\top \hat\varepsilon \over n-p } $$

```{r}
df.residual(fit)  # n-p
sigma(fit)
```
</div>

```{r echo=F}
begin()
to_predict()
the_truth()
```


## Model fitting in R with `lm( )`

Our starting point for modelling is usually a data frame, but the process of getting $y$ and $X$ from the data frame has some complications, and a special "formula" syntax. 

```
        data frame

            ||
            || Magic involving "~" formula syntax happens
            \/

           y, X

            ||
            || Model fitting happens
            \/

        coef, sigma
```

First look at examples then discuss process in detail.




## 

Do section: Single numerical predictor

Do section: Single factor predictor, two levels


## Testing hypotheses

<div style="float: right; width: 15em;">
```{r}
y <- c(3,5)
fit0 <- lm(y ~ 0)
fit1 <- lm(y ~ 1)
```

Two model formulas representing null hypotheses H0 and alternative hypothesis H1.

H0 must *nest* within H1.

$\hat\varepsilon_0$ can't be shorter than $\hat\varepsilon_1$

Is it *surprisingly longer*? Then we reject H0.
</div>

```{r echo=F}
begin()
to_test()
```

## Testing hypotheses

<div style="float: right; width: 15em;">
```{r}
y <- c(3,5)
fit0 <- lm(y ~ 0)
fit1 <- lm(y ~ 1)

anova(fit0, fit1)
```

Here we can not reject the null hypothesis (F(1,1)=16, p=0.156). <span class="tip">*<span class="tiptext">If H0 is true, we expect F to be close to 1. This value of F is much larger than 1. Why can't we reject H0? The problem is that we are very uncertain about $\sigma$ because our residual degrees of freedom is so low. Hence the F(1,1) distribution here has a very long fat tail.</span></span>
</div>

```{r echo=F}
begin()
to_test()
```

## Testing hypotheses -- the gory details

<div style="float: right; width: 15em">
```{r}
df0 <- df.residual(fit0)       # n-p0
RSS0 <- sum(residuals(fit0)^2)

df1 <- df.residual(fit1)       # n-p1
RSS1 <- sum(residuals(fit1)^2)

F <- (RSS0-RSS1)/(df0-df1) / (RSS1/df1) 
F
pf(F, df0-df1, df1, lower.tail=FALSE)
```
</div>
$$
RSS_0 = \hat\varepsilon_0^\top \hat\varepsilon_0 \\
RSS_1 = \hat\varepsilon_1^\top \hat\varepsilon_1
$$

<br>

$$
F = { {RSS_0-RSS_1 \over p_1-p_0} \over {RSS_1 \over n-p_1} }
$$

RSS = Residual Sum of Squares

Is the increase in RSS in H0 merely proportionate 
<br>to the extra degree(s) of freedom?

<div style="font-size: 150%">`anova(fit0, fit1)`<br>handles all this for us.</div>



## A different way to test: linear hypotheses

"linear hypothesis" "general linear hypothesis" "linear restriction" "contrast"

<br>
A **linear hypothesis** test tests an H0 in which some linear combination of coefficients in H1 is restricted to be zero.

<br>
From a linear combination of coefficients we obtain:

>* estimated value (calculated with dot product)
>* Confidence Interval (CI)
>* hypothesis test p-value

<br>
Examples:

>* a **contrast** compares specific levels of an experimental factor<span class="tip">*<span class="tiptext">Terminology is a little tricky here. The limma package refers to linear combinations in general as contrasts.</span></span>
>* the difference of response between two imagined individuals

<br>
Use `multcomp` package.


##

Do section: Multiple factors, many levels


## What is a p-value anyway?

A p-value lets us choose our probability of falsely rejecting a true hypothesis.

If H1 is true, tends towards zero.

If H0 is true, it is a uniformly random number between 0 and 1.

<br>

If we decide to reject hypotheses with $p \leq 0.05$, we are guaranteed a probability of 0.05 of rejecting a hypothesis if it is actually true.

<br>

Suppose we perform 20 such tests. There is a high chance of falsely rejecting at least one of them.

Need to adjust p-values for this problem (see `p.adjust( )`).



## Multiple testing

Suppose some set of hypotheses are true. What chance do we run of rejecting one or more of them?

### Hypotheses are mutually exclusive

> * No problem!

Example: reject all values outside a confidence interval.

### Family-Wise Error Rate (FWER) control

"*Any* false rejection will ruin my argument."

> * Tukey's Honestly Significant Differences
> * Bonferroni correction (etc)

### False Discovery Rate (FDR) control

"I can tolerate a certain *rate* of false rejections amongst rejected hypotheses."

> * Benjamini and Hochberg correction (etc)


## Formulas

"Wilikins-Rogers notation"

Formulas with `~` specify response $y$ and the construction of the model matrix $X$.

Terms refer to columns of a data frame or variables in the environment.

Merely a convenience, if you already have model matrix `X` can just use `y ~ 0 + X`.

<br>

Let's discuss this in more detail.

## Formulas - data types

`numeric` vector is represented as itself.

`logical` vector is converted to 0s and 1s.

`factor` is represented as $n_\text{levels}-1$ columns containing 1s and 0s. 

`matrix` is represented as multiple columns.


## Formulas - factors

$n_\text{levels}-1$ columns of "indicator variables", aka "one-hot" encoding. <span class="tip">*<span class="tiptext">The encoding can be adjusted using the `C( )` function within a formula. The default differs between S and R languages!</span></span>

```{r}
f <- factor(c("a","a","b","b","c","c"))

model.matrix(~ f)
```


## Formulas - intercept

Intercept term `1` is implicit.

Omit with `-1` or `0`.

R tries to be clever about factor encoding if intercept omitted.

```{r}
f <- factor(c("a","a","b","b","c","c"))
```

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ f)
```
</div>

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ 0 + f)
```
</div>


## Formulas - calculations and function calls

Can calculate with function calls, or enclose maths in `I( )`.

Handy functions such as `poly` and `splines::ns` produce matrices for fitting curves.

```{r}
x <- 1:6
```

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ x + I(x^2) + log2(x))
```
</div>

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ poly(x,3))
```
</div>


## Formulas - interactions

`a:b` specifies an interaction between `a` and `b`. 

All pairings of predictors from the two terms are multiplied together.

For logical and factor vectors, this produces the logical "and" of each pairing.
<br>&nbsp;

<br>

```{r}
model.matrix(~ f + x + f:x)
```

## Formulas - interactions

`a*b` is shorthand to also include main effects `a + b + a:b`.

(More obscurely `a/b` is shorthand for `a + a:b`, if `b` only makes sense in the context of each specific `a`.)

<span style="display: inline-block; width: 6em">`(a+b+c)^2`</span> is shorthand for `a + b + c + a:b + a:c + b:c`.
<br>
<span style="display: inline-block; width: 6em">`a*b*c`</span> is shorthand for `a + b + c + a:b + a:c + b:c + a:b:c`.

<br>

```{r}
model.matrix(~ f*x)
```


##

Do section: Gene expression example



## Summary

Should now be able to

>* fit linear models
>* understand the importance of different parts of the fitted model
><ul>
>    <li> coefficients
>    <li> residual standard deviation
>    <li> residual degrees of freedom
></ul>
>* predict using a model
>* test a hypothesis using two nested linear models
>* test a hypothesis and calculate a CI on a linear combination of coefficients
>* test a hypothesis against many sets of responses using limma
>* understand multiple testing correction


## Summary - problems with linear models

Incorrect transformation of y can introduce spurious interactions.

Highly correlated predictors, or completely confounded experimental design.

Problems with residual noise:

>* Non-uniform noise level between conditions
>* Remaining structure
>* Non-normal distribution <div class="tip">*<div class="tiptext">Least squares estimation still has good properties when the noise is not normally distributed, as a way to predict averages, if averages are truly what we want ("BLUE"). However outliers can lead to mis-estimating the accuracy of the model. This is as much about beliefs and past experience as observation -- if outliers are rare, a sample *without* outliers may give a misleading estimate. In the worst case the outliers may be so large the variance doesn't exist, and least squares does completely break down.<br><br>For example it would be important to include outliers like the Global Financial Crisis in an average yearly return on investment, see Nasim Taleb.</div></div>

Various more advanced methods exist if these are present, such as quantile regression and "sandwich" estimation of the accuracy of coefficients and contrasts.


## Extensions to linear models

### Insufficient data or $p > n$

>* regularization with ridge regression and/or LASSO: `glmnet::glmnet( )`
>* Partial Least Squares (PLS) <br>(pick best direction to start moving in and take best leap along it, repeat)
>* Principal Component Regression (PCR)<br>(direction of largest variation in X first)

### Noise not normal (eg binary or count data)

>* Generalized Linear Models: `glm( )`<br>(logistic regression, binomial, Poisson, negative binomial, gamma, etc)
>* Quasi-Likelihood F-test in case of over-dispersion

### Survival time, with censoring

>* Cox survival model: `survival::coxph( )`


## Extensions to linear models

### Noise at a coarser grain than single measurements

Individuals tracked over time, repeated measurements, blocks/batches/plots, etc.

>* Generalized Least Squares (accounts for covariance in noise)
>* Mixed Model (some coefficients are drawn from a random distribution)

### Response is non-linear in predictors

>* Feature engineering: `poly( )`, `splines::ns( )`
>* Local regression (LOESS): `loess( )`
>* Generalized Additive Models (GAM): `mgcv::gam( )`<br>(splines with regularization)
>* Neural network (plug together many logistic regression models)

Also consider tree methods, such as gradient boosted trees.


## Further reading

<div style="float: right; width: 25%; padding 2em;">
```{r echo=F,out.width="100%",fig.align="left"}
knitr::include_graphics("../figures/chambers_and_hastie.jpg")
```
</div>

**Primary reference is:**

Chambers and Hastie (1991) "Statistical Models in S"

<br>
**Also good:**

Faraway (2014) "Linear Models with R"

James, Witten, Hastie and Tibshirani (2013) "An Introduction to Statistical Learning"






